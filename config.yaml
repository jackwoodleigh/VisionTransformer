model:
  n_blocks: 8
  levels: 2
  dim: 128
  features: 64
  window_size: 8
  scale_factor: 4

training:
  learning_rate: 0.00025
  iterations: 500000
  epochs: 100
  batch_size_per_gpu: 8
  accumulation_steps: 2
  model_ema: 0.999
  ema_start_epoch: 2
  criterion:
    CharbonnierLoss: 1.0
    #FFTLoss: 0.05

data:
  data_path: "data"
  training_data_name: "train"
  testing_data_name: "test"
  data_subset: 0
  training_dataset_enlarge_scale: 1
  training_image_size: 256
  validation_image_size: 512
  transform_data: True
  num_dataloader_workers: 12

tools:
  wandb_log: True
  logging_path: "Logs"
  save_model_every_i_epoch: 1
  model_save_name: ""
  load_model_save_name: ""
  load_optimizer: True
  multi_gpu_enable: False
  use_lmdb: True
  use_sub_images: True
  sub_img_col: 4
  sum_img_row: 2

